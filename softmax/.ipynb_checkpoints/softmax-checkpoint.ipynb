{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/precious/Desktop/Machine Learning/Neural Network(easy)/data'\n",
    "kind = 'train'\n",
    "\n",
    "labels_path = os.path.join(path,'%s-labels.idx1-ubyte'% kind)\n",
    "images_path = os.path.join(path,'%s-images.idx3-ubyte'% kind)\n",
    "with open(labels_path, 'rb') as lbpath:\n",
    "    magic, n = struct.unpack('>II',lbpath.read(8))\n",
    "    train_labels = np.fromfile(lbpath,dtype=np.uint8)\n",
    "with open(images_path, 'rb') as imgpath:\n",
    "    magic, num, rows, cols = struct.unpack('>IIII',imgpath.read(16))\n",
    "    train_images = np.fromfile(imgpath,dtype=np.uint8).reshape(len(train_labels), 784)\n",
    "\n",
    "kind = 't10k'\n",
    "\n",
    "labels_path = os.path.join(path,'%s-labels.idx1-ubyte'% kind)\n",
    "images_path = os.path.join(path,'%s-images.idx3-ubyte'% kind)\n",
    "with open(labels_path, 'rb') as lbpath:\n",
    "    magic, n = struct.unpack('>II',lbpath.read(8))\n",
    "    test_labels = np.fromfile(lbpath,dtype=np.uint8)\n",
    "with open(images_path, 'rb') as imgpath:\n",
    "    magic, num, rows, cols = struct.unpack('>IIII',imgpath.read(16))\n",
    "    test_images = np.fromfile(imgpath,dtype=np.uint8).reshape(len(test_labels), 784)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_img(img_data, img_label, count, rows=1, cols=0):\n",
    "    if not cols:\n",
    "        cols = int(count / rows)\n",
    "    fig=plt.figure(figsize=(8,8))\n",
    "    fig.subplots_adjust(left=0,right=1,bottom=0,top=1,hspace=0.05,wspace=0.05)\n",
    "    for i in range(count):\n",
    "        images = np.reshape(img_data[i], [28,28])\n",
    "        ax=fig.add_subplot(rows,cols,i+1,xticks=[],yticks=[])\n",
    "        ax.imshow(images,cmap=plt.cm.binary,interpolation='nearest')\n",
    "        ax.text(0,7,str(img_label[i]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_img(train_images, train_labels, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m dev \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dev = 'mps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = (np.asfarray(train_images) / 255.0 * 0.99) + 0.01\n",
    "test_images = (np.asfarray(test_images) / 255.0 * 0.99) + 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = torch.tensor(train_images, dtype=torch.float)\n",
    "test_tensor = torch.tensor(test_images, dtype=torch.float)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.int64)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_tensor = torch.nn.functional.one_hot(train_labels_tensor, num_classes=10).float()\n",
    "test_labels_tensor = torch.nn.functional.one_hot(test_labels_tensor, num_classes=10).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "def make_iters(tensor, labels, batch_size, num_workers=8):\n",
    "    set = data.TensorDataset(tensor, labels)\n",
    "    iter = data.DataLoader(set, batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    return iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "# train_tensor = train_tensor[:100]\n",
    "# train_labels_tensor = train_labels_tensor[:100]\n",
    "# test_tensor = test_tensor[:10]\n",
    "# test_labels_tensor = test_labels_tensor[:10]\n",
    "\n",
    "train_iter = make_iters(train_tensor, train_labels_tensor, batch_size)\n",
    "test_iter = make_iters(test_tensor, test_labels_tensor, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(F.softmax(x, dim=0))\n",
    "        return x\n",
    "\n",
    "net = Net().to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,n_input,n_hidden,n_output):\n",
    "        super(Net,self).__init__()\n",
    "        self.hidden1 = nn.Linear(n_input,n_hidden)\n",
    "        self.hidden2 = nn.Linear(n_hidden,n_hidden)\n",
    "        self.predict = nn.Linear(n_hidden,n_output)\n",
    "    def forward(self,input):\n",
    "        out = self.hidden1(input)\n",
    "        out = F.relu(out)\n",
    "        out = self.hidden2(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        out = self.predict(out)\n",
    "        # out = F.softmax(out, dim=0)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "net = Net(28*28, 28, 10).to(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor(range(28*28),dtype=torch.float).to(dev)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=1\n",
    "                    #   , momentum=0.9\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def accuracy(y_hat, y):\n",
    "#     \"\"\"Calculate the number of the right samples\"\"\"\n",
    "#     if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "#         y_hat = y_hat.argmax(axis=0)\n",
    "#     cmp = y_hat.type(y.dtype) == y\n",
    "#     return float(cmp.type(y.dtype).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Accumulator:\n",
    "#     \"\"\"Summing over n variables\"\"\"\n",
    "#     def __init__(self, n):\n",
    "#         self.data = [0.0] * n\n",
    "\n",
    "#     def add(self, *args):\n",
    "#         self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "    \n",
    "#     def reset(self):\n",
    "#         self.data = [0.0] * len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_accuracy(net, data_iter):\n",
    "#     \"\"\"Computes the precision of the model on the specified dataset.\"\"\"\n",
    "#     if isinstance(net, torch.nn.Module):\n",
    "#         net.eval()   # evaluation models\n",
    "#     metric = Accumulator(2)\n",
    "#     for X, y in data_iter:\n",
    "#         metric.add(accuracy(net(X.to(dev)), y.to(dev)), y.numel())\n",
    "#     return metric[0] / metric[1]\n",
    "def evaluate_accuracy(net, data, data_labels):\n",
    "    temp = net(data)\n",
    "    temp = torch.max(temp, dim=1).indices\n",
    "    return ((temp.eq(torch.max(data_labels, dim=1).indices)==True).sum() / len(data)).item()\n",
    "\n",
    "evaluate_accuracy(net, train_tensor.to(dev), train_labels_tensor.to(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import append\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "train_acc = evaluate_accuracy(net, train_tensor.to(dev), train_labels_tensor.to(dev))\n",
    "test_acc = evaluate_accuracy(net, test_tensor.to(dev), test_labels_tensor.to(dev))\n",
    "i = 0\n",
    "\n",
    "y = np.array([[train_acc, test_acc]])\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    # i = 0\n",
    "    for  batch_datas, batch_labels in train_iter:\n",
    "        i += batch_size\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(batch_datas.to(dev))\n",
    "        loss = criterion(outputs, batch_labels.to(dev))\n",
    "        # accumulation_steps = 10\n",
    "        # loss = loss / accumulation_steps                # Normalize our loss (if averaged)\n",
    "        # loss.backward()                                 # Backward pass\n",
    "        # if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "        #     optimizer.step()                            # Now we can do an optimizer step\n",
    "        #     net.zero_grad()                             # Reset gradients tensors\n",
    "        #     if (i+1) % evaluation_steps == 0:           # Evaluate the model when we...\n",
    "        #         criterion()                             # ...have no gradients accumulated\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        # running_loss += loss.item()\n",
    "        # if i % 30000 == 1:    # print every 2000 mini-batches\n",
    "        #     print('[%d, %5d] loss: %.3f' %\n",
    "        #           (epoch + 1, i + 1, running_loss / (2000/batch_size)))\n",
    "        #     running_loss = 0.0\n",
    "        # running_loss = loss.item()\n",
    "    \n",
    "    train_acc = evaluate_accuracy(net, train_tensor.to(dev), train_labels_tensor.to(dev))\n",
    "    test_acc = evaluate_accuracy(net, test_tensor.to(dev), test_labels_tensor.to(dev))\n",
    "    y = np.insert(y, len(y), np.array([train_acc, test_acc]), axis=0)\t# 添加i的平方到y轴的数据中\n",
    "\n",
    "    # print('train_accuracy={}'.format(evaluate_accuracy(net, train_iter)))\n",
    "    print('test_accuracy={}'.format(test_acc))\n",
    "    display.clear_output(wait=True)\n",
    "    # print(y.shape)\n",
    "    plt.plot(range(epoch+2), y[:,0], label='train_acc')\n",
    "    plt.plot(range(epoch+2), y[:,1], label='test_acc')\n",
    "    # plt.plot(y[:,2], label='loss')\n",
    "\n",
    "    # plt.ylabel('accuracy')\n",
    "    plt.ylim([0,1])\n",
    "    plt.xlim([0, num_epochs])\n",
    "    plt.xlabel('batch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.pause(0.1)  # 暂停一段时间，不然画的太快会卡住显示不出来\n",
    "    \n",
    "    # print('train_accuracy={}'.format(evaluate_accuracy(net, train_iter)))\n",
    "    # print('test_accuracy={}'.format(evaluate_accuracy(net, test_iter)))\n",
    "    # animator.add(i + 1, train_metrics + (test_acc,))\n",
    "\n",
    "# train_loss, train_acc = train_metrics\n",
    "\n",
    "# plt.plot(accuracy_list)\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net(test_tensor.to(dev)).max(dim=1).indices.eq(torch.tensor(test_labels).to(dev))\n",
    "evaluate_accuracy(net, test_tensor.to(dev), test_labels_tensor.to(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_pic(picName):\n",
    "    #打开传入的原始图片\n",
    "    img = Image.open(picName)\n",
    "    #为符合要求，把图片resize成28*28，用消除锯齿的方法\n",
    "    relm = img.resize((28, 28), Image.Resampling.LANCZOS)\n",
    "\n",
    "    #变成灰度图并转换为矩阵形式\n",
    "\n",
    "    im_arr = np.array(relm.convert(\"L\"))\n",
    "    threshold = 110\n",
    "\n",
    "    #给图片反色，因为要求输入黑底白字，输入的是白底黑字，并进行二值化处理\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            im_arr[i][j]= 255-im_arr[i][j]\n",
    "            if(im_arr[i][j]<threshold):\n",
    "                im_arr[i][j] = 0\n",
    "            else:im_arr[i][j] = 255\n",
    "\n",
    "    nm_arr = im_arr.reshape([1, 784])\n",
    "    nm_arr = nm_arr.astype(np.float32)\n",
    "    img_ready = np.multiply(nm_arr, 1.0/255.0)\n",
    "    return img_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_path = 'mydata/'\n",
    "pic_list = []\n",
    "\n",
    "for i in range(10):\n",
    "    pic_list.append(pre_pic(pic_path + '{}.png'.format(i))[0])\n",
    "\n",
    "len(pic_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_tensor = torch.tensor(np.array(pic_list))\n",
    "pic_labels = np.array(range(len(pic_tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(pic_tensor, pic_labels, len(pic_tensor))\n",
    "print(torch.max(net(pic_tensor.to(dev)), dim=1).indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yao_path = 'mydata/yao/'\n",
    "\n",
    "yao_number = []\n",
    "for i in range(7):\n",
    "    yao_number.append(pre_pic(yao_path + '{}.png'.format(i+1))[0])\n",
    "\n",
    "len(yao_number[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yao_tensor = torch.tensor(np.array(yao_number))\n",
    "yao_labels = np.array([5,2,0,1,3,1,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(yao_tensor, yao_labels, len(yao_tensor))\n",
    "print(torch.max(net(yao_tensor.to(dev)), dim=1).indices)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "275a4a4caac3b599141279da828175b7d7e8e743fa5a3234eb41dae0868b9f4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
