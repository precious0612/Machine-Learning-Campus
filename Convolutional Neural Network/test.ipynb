{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-05T11:30:31.276974Z",
     "end_time": "2023-05-05T11:30:33.562608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing PlaidML. Make sure you follow this order\n",
    "# import plaidml.keras\n",
    "# plaidml.keras.install_backend()\n",
    "# import os\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "# then you can write your codes\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, PReLU, AveragePooling2D, ZeroPadding2D\n",
    "from keras import backend as K\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T11:30:33.566747Z",
     "end_time": "2023-05-05T11:30:33.920988Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T11:30:35.489952Z",
     "end_time": "2023-05-05T11:30:35.749526Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(train_imgs, train_labels), (test_imgs, test_labels) = mnist.load_data(path='mnist.npz')\n",
    "\n",
    "# 灰度归一化\n",
    "train_imgs = train_imgs / 255\n",
    "test_imgs = test_imgs / 255\n",
    "\n",
    "# 调整维度，按照 28*28 大小拆分图片数据\n",
    "\n",
    "train_imgs = train_imgs.reshape(-1, 28, 28, 1)\n",
    "test_imgs = test_imgs.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# 调整标签为onehot\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T11:30:36.284508Z",
     "end_time": "2023-05-05T11:30:39.865324Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_imgs = np.resize(train_imgs, (60000, 224, 224, 1))\n",
    "test_imgs = np.resize(test_imgs, (10000, 224, 224, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "残差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T11:30:39.867831Z",
     "end_time": "2023-05-05T11:30:39.869884Z"
    }
   },
   "outputs": [],
   "source": [
    "def Conv_BN_Relu(filters, kernel_size, strides, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters, kernel_size, strides=strides, padding='same', input_shape=input_shape))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(PReLU('ones'))\n",
    "    model.add(MaxPooling2D(pool_size=3, strides=1))\n",
    "    return model\n",
    "\n",
    "def resiidual_a(pre_model, filters, input_shape, pre_index=0, index=3):\n",
    "    model = Sequential()\n",
    "    model.add(Conv_BN_Relu(filters, 3, 1, input_shape))\n",
    "    if pre_model.output_shape == model.output_shape:\n",
    "        print(pre_model.layers[pre_index], model.layers[index])\n",
    "        model.add([pre_model.layers[pre_index], model.layers[index]])\n",
    "    else:\n",
    "        return model\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T11:30:39.873247Z",
     "end_time": "2023-05-05T11:30:40.345496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2 Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 11:30:39.875370: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-05 11:30:39.875500: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(96, kernel_size=11, strides=4, padding='same'\n",
    "                 , input_shape=(224,224,1)\n",
    "                 ))\n",
    "model.add(PReLU('ones'))\n",
    "model.add(MaxPooling2D(pool_size=3, strides=2))\n",
    "model.add(resiidual_a(model, 256, model.output_shape[1:], 2, 3))\n",
    "model.add(Conv2D(128, kernel_size=3, strides=1))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(PReLU('ones'))\n",
    "model.add(MaxPooling2D(pool_size=3, strides=1))\n",
    "model.add(resiidual_a(model, 256, model.output_shape[1:], 6, 7))\n",
    "model.add(Conv2D(384, kernel_size=3, strides=1))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(PReLU('ones'))\n",
    "model.add(MaxPooling2D(pool_size=3, strides=1))\n",
    "model.add(Conv2D(256, kernel_size=3, strides=1))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(PReLU('ones'))\n",
    "model.add(MaxPooling2D(pool_size=3, strides=1))\n",
    "model.add(resiidual_a(model, 384, model.output_shape[1:], 18, 21))\n",
    "\n",
    "# 构建分类器\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 11:30:52.257591: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-05-05 11:30:52.736084: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283/469 [=================>............] - ETA: 1:16 - loss: 2.3029 - accuracy: 0.1088"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 训练模型\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_imgs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_labels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# 评估模型\u001B[39;00m\n\u001B[1;32m      4\u001B[0m evaluate_res \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mevaluate(x\u001B[38;5;241m=\u001B[39mtest_imgs, y\u001B[38;5;241m=\u001B[39mtest_labels, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m128\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py:1414\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1412\u001B[0m logs \u001B[38;5;241m=\u001B[39m tmp_logs  \u001B[38;5;66;03m# No error, now safe to assign to logs.\u001B[39;00m\n\u001B[1;32m   1413\u001B[0m end_step \u001B[38;5;241m=\u001B[39m step \u001B[38;5;241m+\u001B[39m data_handler\u001B[38;5;241m.\u001B[39mstep_increment\n\u001B[0;32m-> 1414\u001B[0m \u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_train_batch_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43mend_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1415\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop_training:\n\u001B[1;32m   1416\u001B[0m   \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/callbacks.py:438\u001B[0m, in \u001B[0;36mCallbackList.on_train_batch_end\u001B[0;34m(self, batch, logs)\u001B[0m\n\u001B[1;32m    431\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001B[39;00m\n\u001B[1;32m    432\u001B[0m \n\u001B[1;32m    433\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m    434\u001B[0m \u001B[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001B[39;00m\n\u001B[1;32m    435\u001B[0m \u001B[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001B[39;00m\n\u001B[1;32m    436\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    437\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_call_train_batch_hooks:\n\u001B[0;32m--> 438\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mModeKeys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTRAIN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mend\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/callbacks.py:297\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook\u001B[0;34m(self, mode, hook, batch, logs)\u001B[0m\n\u001B[1;32m    295\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_batch_begin_hook(mode, batch, logs)\n\u001B[1;32m    296\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m hook \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 297\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_end_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    298\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    299\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    300\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUnrecognized hook: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Expected values are [\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbegin\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/callbacks.py:318\u001B[0m, in \u001B[0;36mCallbackList._call_batch_end_hook\u001B[0;34m(self, mode, batch, logs)\u001B[0m\n\u001B[1;32m    315\u001B[0m   batch_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_start_time\n\u001B[1;32m    316\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_times\u001B[38;5;241m.\u001B[39mappend(batch_time)\n\u001B[0;32m--> 318\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_hook_helper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    320\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_times) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_batches_for_timing_check:\n\u001B[1;32m    321\u001B[0m   end_hook_name \u001B[38;5;241m=\u001B[39m hook_name\n",
      "File \u001B[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/callbacks.py:356\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook_helper\u001B[0;34m(self, hook_name, batch, logs)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks:\n\u001B[1;32m    355\u001B[0m   hook \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(callback, hook_name)\n\u001B[0;32m--> 356\u001B[0m   \u001B[43mhook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_timing:\n\u001B[1;32m    359\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m hook_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hook_times:\n",
      "File \u001B[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/callbacks.py:1034\u001B[0m, in \u001B[0;36mProgbarLogger.on_train_batch_end\u001B[0;34m(self, batch, logs)\u001B[0m\n\u001B[1;32m   1033\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_train_batch_end\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch, logs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m-> 1034\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_batch_update_progbar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/callbacks.py:1106\u001B[0m, in \u001B[0;36mProgbarLogger._batch_update_progbar\u001B[0;34m(self, batch, logs)\u001B[0m\n\u001B[1;32m   1102\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseen \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m add_seen\n\u001B[1;32m   1104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   1105\u001B[0m   \u001B[38;5;66;03m# Only block async when verbose = 1.\u001B[39;00m\n\u001B[0;32m-> 1106\u001B[0m   logs \u001B[38;5;241m=\u001B[39m \u001B[43mtf_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msync_to_numpy_or_python_type\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1107\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprogbar\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseen, \u001B[38;5;28mlist\u001B[39m(logs\u001B[38;5;241m.\u001B[39mitems()), finalize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/tf_utils.py:607\u001B[0m, in \u001B[0;36msync_to_numpy_or_python_type\u001B[0;34m(tensors)\u001B[0m\n\u001B[1;32m    604\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\n\u001B[1;32m    605\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39mndim(t) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m t\n\u001B[0;32m--> 607\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_structure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_to_single_numpy_or_python_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/nest.py:916\u001B[0m, in \u001B[0;36mmap_structure\u001B[0;34m(func, *structure, **kwargs)\u001B[0m\n\u001B[1;32m    912\u001B[0m flat_structure \u001B[38;5;241m=\u001B[39m (flatten(s, expand_composites) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m structure)\n\u001B[1;32m    913\u001B[0m entries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mflat_structure)\n\u001B[1;32m    915\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pack_sequence_as(\n\u001B[0;32m--> 916\u001B[0m     structure[\u001B[38;5;241m0\u001B[39m], [func(\u001B[38;5;241m*\u001B[39mx) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m entries],\n\u001B[1;32m    917\u001B[0m     expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites)\n",
      "File \u001B[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/nest.py:916\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    912\u001B[0m flat_structure \u001B[38;5;241m=\u001B[39m (flatten(s, expand_composites) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m structure)\n\u001B[1;32m    913\u001B[0m entries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mflat_structure)\n\u001B[1;32m    915\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pack_sequence_as(\n\u001B[0;32m--> 916\u001B[0m     structure[\u001B[38;5;241m0\u001B[39m], [\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m entries],\n\u001B[1;32m    917\u001B[0m     expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites)\n",
      "File \u001B[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/tf_utils.py:601\u001B[0m, in \u001B[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m    598\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_to_single_numpy_or_python_type\u001B[39m(t):\n\u001B[1;32m    599\u001B[0m   \u001B[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001B[39;00m\n\u001B[1;32m    600\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(t, tf\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m--> 601\u001B[0m     t \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    602\u001B[0m   \u001B[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001B[39;00m\n\u001B[1;32m    603\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(t, (np\u001B[38;5;241m.\u001B[39mndarray, np\u001B[38;5;241m.\u001B[39mgeneric)):\n",
      "File \u001B[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1159\u001B[0m, in \u001B[0;36m_EagerTensorBase.numpy\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1136\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001B[39;00m\n\u001B[1;32m   1137\u001B[0m \n\u001B[1;32m   1138\u001B[0m \u001B[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1156\u001B[0m \u001B[38;5;124;03m    NumPy dtype.\u001B[39;00m\n\u001B[1;32m   1157\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1158\u001B[0m \u001B[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001B[39;00m\n\u001B[0;32m-> 1159\u001B[0m maybe_arr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m   1160\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m maybe_arr\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(maybe_arr, np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;28;01melse\u001B[39;00m maybe_arr\n",
      "File \u001B[0;32m~/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1125\u001B[0m, in \u001B[0;36m_EagerTensorBase._numpy\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1123\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_numpy\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m   1124\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_numpy_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1126\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_status_to_exception(e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "history = model.fit(train_imgs, train_labels, epochs=16, batch_size=128)\n",
    "# 评估模型\n",
    "evaluate_res = model.evaluate(x=test_imgs, y=test_labels, batch_size=128)\n",
    "\n",
    "# 输出评估结果\n",
    "print(evaluate_res)\n",
    "\n",
    "# 输出模型的结构信息\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 借助 history 对象了解训练过程\n",
    "history_dict = history.history\n",
    "acc = history_dict['acc']\n",
    "loss = history_dict['loss']\n",
    "\n",
    "# 借助 Matplotlib 绘制图像\n",
    "plt.plot(range(1, len(acc)+1), acc, 'b--')\n",
    "plt.plot(range(1, len(loss)+1), loss, 'r-')\n",
    "\n",
    "# 显示图例\n",
    "plt.legend(['accuracy', 'loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_imgs = train_imgs.copy().resize(train_imgs.shape[0], 224, 224, 1)\n",
    "test_imgs = test_imgs.copy().resize(test_imgs.shape[0], 224, 224, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Cifar 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(train_imgs, train_labels), (test_imgs, test_labels) = cifar10.load_data()\n",
    "\n",
    "train_imgs = train_imgs / 255\n",
    "test_imgs = test_imgs / 255\n",
    "\n",
    "train_imgs = train_imgs.reshape(-1, 32, 32, 3)\n",
    "test_imgs = test_imgs.reshape(-1, 32, 32, 3)\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_26 (Conv2D)           (None, 30, 30, 96)        2688      \n",
      "_________________________________________________________________\n",
      "p_re_lu_26 (PReLU)           (None, 30, 30, 96)        86400     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 29, 29, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 27, 27, 512)       442880    \n",
      "_________________________________________________________________\n",
      "p_re_lu_27 (PReLU)           (None, 27, 27, 512)       373248    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 11, 11, 256)       1179904   \n",
      "_________________________________________________________________\n",
      "p_re_lu_28 (PReLU)           (None, 11, 11, 256)       30976     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 3, 3, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "p_re_lu_29 (PReLU)           (None, 3, 3, 512)         4608      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 4,063,374\n",
      "Trainable params: 4,063,374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(96,(3,3), input_shape=(32,32,3)))\n",
    "model.add(PReLU('ones'))\n",
    "# model.add(BatchNormalization(axis=-1,\n",
    "#     momentum=0.99,\n",
    "#     epsilon=0.001,))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=1))\n",
    "\n",
    "model.add(Conv2D(512,(3,3)))\n",
    "model.add(PReLU('ones'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256,(3,3)))\n",
    "model.add(PReLU('ones'))\n",
    "# model.add(BatchNormalization(axis=-1,\n",
    "#     momentum=0.99,\n",
    "#     epsilon=0.001,))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(512,(3,3)))\n",
    "model.add(PReLU('ones'))\n",
    "# model.add(BatchNormalization(0.5))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "50000/50000 [==============================] - 166s 3ms/step - loss: 0.6499 - acc: 0.7728\n",
      "Epoch 2/32\n",
      "50000/50000 [==============================] - 157s 3ms/step - loss: 0.6178 - acc: 0.7819\n",
      "Epoch 3/32\n",
      "50000/50000 [==============================] - 189s 4ms/step - loss: 0.5879 - acc: 0.7908\n",
      "Epoch 4/32\n",
      "50000/50000 [==============================] - 230s 5ms/step - loss: 0.5649 - acc: 0.8000\n",
      "Epoch 5/32\n",
      "50000/50000 [==============================] - 267s 5ms/step - loss: 0.5345 - acc: 0.8118\n",
      "Epoch 6/32\n",
      "50000/50000 [==============================] - 316s 6ms/step - loss: 0.5140 - acc: 0.8198\n",
      "Epoch 7/32\n",
      "50000/50000 [==============================] - 326s 7ms/step - loss: 0.4795 - acc: 0.8317\n",
      "Epoch 8/32\n",
      "50000/50000 [==============================] - 322s 6ms/step - loss: 0.4578 - acc: 0.8400\n",
      "Epoch 9/32\n",
      "50000/50000 [==============================] - 304s 6ms/step - loss: 0.4279 - acc: 0.8487\n",
      "Epoch 10/32\n",
      "50000/50000 [==============================] - 284s 6ms/step - loss: 0.4056 - acc: 0.8571\n",
      "Epoch 11/32\n",
      "50000/50000 [==============================] - 332s 7ms/step - loss: 0.3819 - acc: 0.8639\n",
      "Epoch 12/32\n",
      "50000/50000 [==============================] - 339s 7ms/step - loss: 0.3583 - acc: 0.8726\n",
      "Epoch 13/32\n",
      "50000/50000 [==============================] - 376s 8ms/step - loss: 0.3419 - acc: 0.8782\n",
      "Epoch 14/32\n",
      "50000/50000 [==============================] - 359s 7ms/step - loss: 0.3154 - acc: 0.8888\n",
      "Epoch 15/32\n",
      "50000/50000 [==============================] - 356s 7ms/step - loss: 0.3016 - acc: 0.8937\n",
      "Epoch 16/32\n",
      "50000/50000 [==============================] - 360s 7ms/step - loss: 0.2947 - acc: 0.8970\n",
      "Epoch 17/32\n",
      "50000/50000 [==============================] - 359s 7ms/step - loss: 0.2652 - acc: 0.9074\n",
      "Epoch 18/32\n",
      "50000/50000 [==============================] - 356s 7ms/step - loss: 0.2580 - acc: 0.9085\n",
      "Epoch 19/32\n",
      "50000/50000 [==============================] - 419s 8ms/step - loss: 0.2498 - acc: 0.9130\n",
      "Epoch 20/32\n",
      "50000/50000 [==============================] - 378s 8ms/step - loss: 0.2181 - acc: 0.9226\n",
      "Epoch 21/32\n",
      "50000/50000 [==============================] - 367s 7ms/step - loss: 0.2112 - acc: 0.9272\n",
      "Epoch 22/32\n",
      "50000/50000 [==============================] - 214s 4ms/step - loss: 0.2090 - acc: 0.9269\n",
      "Epoch 23/32\n",
      "50000/50000 [==============================] - 191s 4ms/step - loss: 0.1915 - acc: 0.9331\n",
      "Epoch 24/32\n",
      "50000/50000 [==============================] - 197s 4ms/step - loss: 0.1733 - acc: 0.9395\n",
      "Epoch 25/32\n",
      "26880/50000 [===============>..............] - ETA: 1:30 - loss: 0.1672 - acc: 0.9418"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "history = model.fit(train_imgs, train_labels, epochs=32, batch_size=64, shuffle=True)\n",
    "# 评估模型\n",
    "evaluate_res = model.evaluate(x=test_imgs, y=test_labels, batch_size=64)\n",
    "\n",
    "# 输出评估结果\n",
    "print(evaluate_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 借助 history 对象了解训练过程\n",
    "history_dict = history.history\n",
    "acc = history_dict['acc']\n",
    "loss = history_dict['loss']\n",
    "\n",
    "# 借助 Matplotlib 绘制图像\n",
    "plt.plot(range(1, len(acc)+1), acc, 'b--')\n",
    "plt.plot(range(1, len(loss)+1), loss, 'r-')\n",
    "\n",
    "# 显示图例\n",
    "plt.legend(['accuracy', 'loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Fashion_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(train_imgs, train_labels), (test_imgs, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# 灰度归一化\n",
    "train_imgs = train_imgs / 255\n",
    "test_imgs = test_imgs / 255\n",
    "\n",
    "# 调整维度，按照 28*28 大小拆分图片数据\n",
    "\n",
    "train_imgs = train_imgs.reshape(-1, 28, 28, 1)\n",
    "test_imgs = test_imgs.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# 调整标签为onehot\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, AveragePooling2D, Flatten, GlobalAveragePooling2D, Dense, Dropout\n",
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Inception_block(input_layer, f1, f2_conv1, f2_conv3, f3_conv1, f3_conv5, f4, f5):\n",
    "  # Input:\n",
    "  # - f1: number of filters of the 1x1 convolutional layer in the first path\n",
    "  # - f2_conv1, f2_conv3 are number of filters corresponding to the 1x1 and 3x3 convolutional layers in the second path\n",
    "  # - f3_conv1, f3_conv5 are the number of filters corresponding to the 1x1 and 5x5  convolutional layer in the third path\n",
    "  # - f4: number of filters of the 1x1 convolutional layer in the fourth path\n",
    "\n",
    "  # 1st path:\n",
    "  path1 = Conv2D(filters=f1, kernel_size = (1,1), padding = 'same')(input_layer)\n",
    "  path1 = PReLU('one')(path1)\n",
    "\n",
    "  # 2nd path\n",
    "  path2 = Conv2D(filters = f2_conv1, kernel_size = (1,1), padding = 'same')(input_layer)\n",
    "  path2 = PReLU('one')(path2)\n",
    "  path2 = Conv2D(filters = f2_conv3, kernel_size = (3,3), padding = 'same')(path2)\n",
    "  path2 = PReLU('one')(path2)\n",
    "\n",
    "  # 3rd path\n",
    "  path3 = Conv2D(filters = f3_conv1, kernel_size = (1,1), padding = 'same')(input_layer)\n",
    "  path3 = PReLU('one')(path3)\n",
    "  path3 = Conv2D(filters = f3_conv5, kernel_size = (5,5), padding = 'same')(path3)\n",
    "  path3 = PReLU('one')(path3)\n",
    "\n",
    "  # 4th path\n",
    "  path4 = MaxPooling2D((3,3), strides= (1,1), padding = 'same')(input_layer)\n",
    "  path4 = Conv2D(filters = f4, kernel_size = (1,1), padding = 'same')(path4)\n",
    "  path4 = PReLU('one')(path4)\n",
    "\n",
    "  # 5th path\n",
    "  path5 = AveragePooling2D(3, strides=1, padding = 'same')(input_layer)\n",
    "  path5 = Conv2D(filters = f5, kernel_size=1, padding = 'same')(path5)\n",
    "  path5 = PReLU('ones')(path5)\n",
    "\n",
    "  output_layer = concatenate([path1, path2, path3, path4, path5], axis = -1)\n",
    "\n",
    "  return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GoogLeNet():\n",
    "\n",
    "  input_layer = Input(shape = (28, 28, 1))\n",
    "  X = Conv2D(filters = 64, kernel_size = (3,3), strides = 1, padding = 'valid')(input_layer)\n",
    "  X = PReLU('ones')(X)\n",
    "  X = MaxPooling2D(pool_size = (3,3), strides = 2)(X)\n",
    "  X = Conv2D(filters = 192, kernel_size = (3,3), padding = 'same')(X)\n",
    "  X = PReLU('ones')(X)\n",
    "  X = MaxPooling2D(pool_size= (3,3), strides = 2)(X)\n",
    "\n",
    "  X = Inception_block(X, f1 = 64, f2_conv1 = 96, f2_conv3 = 128, f3_conv1 = 16, f3_conv5 = 32, f4 = 32, f5=32)\n",
    "  X = Inception_block(X, f1 = 128, f2_conv1 = 128, f2_conv3 = 192, f3_conv1 = 32, f3_conv5 = 96, f4 = 64, f5=32)\n",
    "  # X = MaxPooling2D(pool_size= (3,3), strides = 2)(X)\n",
    "  # X = Inception_block(X, f1 = 192, f2_conv1 = 96, f2_conv3 = 208, f3_conv1 = 16, f3_conv5 = 48, f4 = 64)\n",
    "\n",
    "  # X = Inception_block(X, f1 = 160, f2_conv1 = 112, f2_conv3 = 224, f3_conv1 = 24, f3_conv5 = 64, f4 = 64)\n",
    "  # X = Inception_block(X, f1 = 128, f2_conv1 = 128, f2_conv3 = 256, f3_conv1 = 24, f3_conv5 = 64, f4 = 64)\n",
    "  # X = Inception_block(X, f1 = 112, f2_conv1 = 144, f2_conv3 = 288, f3_conv1 = 32, f3_conv5 = 64, f4 = 64)\n",
    "\n",
    "  X = Inception_block(X, f1 = 256, f2_conv1 = 160, f2_conv3 = 320, f3_conv1 = 32, f3_conv5 = 128, f4 = 128, f5=64)\n",
    "  X = MaxPooling2D(pool_size = (3,3), strides = 2)(X)\n",
    "  X = Inception_block(X, f1 = 256, f2_conv1 = 160, f2_conv3 = 320, f3_conv1 = 32, f3_conv5 = 128, f4 = 128, f5=64)\n",
    "  X = Inception_block(X, f1 = 384, f2_conv1 = 192, f2_conv3 = 384, f3_conv1 = 48, f3_conv5 = 128, f4 = 128, f5=64)\n",
    "\n",
    "  # Global Average pooling layer\n",
    "  X = GlobalAveragePooling2D(name = 'GAPL')(X)\n",
    "  X = Dropout(0.4)(X)\n",
    "  X = Dense(10, activation = 'softmax')(X)\n",
    "\n",
    "  # model\n",
    "  model = Model(input_layer, [X], name = 'GoogLeNet')\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = GoogLeNet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "history = model.fit(train_imgs, train_labels, epochs=24, batch_size=64, shuffle=True)\n",
    "# 评估模型\n",
    "evaluate_res = model.evaluate(x=test_imgs, y=test_labels, batch_size=64)\n",
    "\n",
    "# 输出评估结果\n",
    "print(evaluate_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "acc = history_dict['acc']\n",
    "loss = history_dict['loss']\n",
    "\n",
    "# 借助 Matplotlib 绘制图像\n",
    "plt.plot(range(1, len(acc)+1), acc, 'b--')\n",
    "plt.plot(range(1, len(loss)+1), loss, 'r-')\n",
    "\n",
    "# 显示图例\n",
    "plt.legend(['accuracy', 'loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "275a4a4caac3b599141279da828175b7d7e8e743fa5a3234eb41dae0868b9f4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
